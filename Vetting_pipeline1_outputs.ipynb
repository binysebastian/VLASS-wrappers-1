{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cb8539a",
   "metadata": {},
   "source": [
    "# Initializing the data\n",
    "\n",
    "Change the link to manifest.csv, table 1,2 and 3 in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c44d6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83685413",
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest_csv='media/manifests/manifest.csv'\n",
    "table1=glob('data/products/VLASS*Catalogue*.csv')[-1]\n",
    "table3=glob('data/products/CIRADA_VLASS*table3_subtile_info_*.csv')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d6c798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir qa_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800d1cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "table1_saved_stats='saved_counts/table1_stats.csv'\n",
    "table2_saved_stats='saved_counts/VLASS2SECIR_hosts_stats.csv'\n",
    "\n",
    "table1data=pd.read_csv(table1)\n",
    "\n",
    "table3data=pd.read_csv(table3)\n",
    "\n",
    "typen=table1data.Component_name.iloc[0].split(' ')[0][6:8]\n",
    "epoch=table1data.Component_name.iloc[0].split(' ')[0][5]\n",
    "\n",
    "typesav,epochsav='SE','2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0988e03f",
   "metadata": {},
   "source": [
    "## Checking table sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15691688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def validate_table_sizes(manifest_path, table1, table3):\n",
    "    # Load the manifest file and count the lines\n",
    "    with open(manifest_path, 'r') as file:\n",
    "        n = len(file.readlines())\n",
    "\n",
    "    # Load Table 1 and Table 3\n",
    "#     table1 = pd.read_csv(table1_path)\n",
    "#     table3 = pd.read_csv(table3_path)\n",
    "\n",
    "    # Calculate the ratios\n",
    "    ratio_table1 = len(table1) / (n - 1)\n",
    "    ratio_table3 = len(table3) / (n - 1)\n",
    "    fout=open('qa_outputs/sizes.txt',mode='w')\n",
    "    # Check if the ratios are within the expected range\n",
    "    print(f\"Table 1 Ratio: {ratio_table1}\")\n",
    "    fout.write(f\"Table 1 Ratio: {ratio_table1}\\n\")\n",
    "    print(f\"Table 3 Ratio: {ratio_table3}\")\n",
    "    fout.write(f\"Table 3 Ratio: {ratio_table3}\\n\")\n",
    "    if 0.95<ratio_table3 < 1.05 and 86 <= ratio_table1 <= 96:\n",
    "        print(\"Table sizes are within the expected range.\")\n",
    "        fout.write(\"Table sizes are within the expected range.\\n\")\n",
    "    else:\n",
    "        print(\"Table sizes are NOT within the expected range.\")\n",
    "        fout.write(\"Table sizes are NOT within the expected range.\\n\")\n",
    "\n",
    "# Usage example\n",
    "validate_table_sizes(manifest_csv,table1data,table3data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546127a1",
   "metadata": {},
   "source": [
    "# Checking stats of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f713e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table1databackup=table1data.copy()\n",
    "import pandas as pd\n",
    "\n",
    "def compare_future_table_with_stats(future_table, stats_file_path):\n",
    "    # Columns to be excluded from the comparison\n",
    "    excluded_columns = [\n",
    "        'Alpha_err_corr', 'Alpha_quality_flag', 'Alpha_err_(TT1/TT0)', \n",
    "        'Best_neuron_y', 'Best_neuron_x', 'Neuron_dist', 'P_sidelobe'\n",
    "    ]\n",
    "\n",
    "    # Specific columns to check\n",
    "    columns_to_check = ['Total_flux', 'Peak_flux', 'DC_Maj']\n",
    "\n",
    "    # Load the saved statistics and the future table\n",
    "    saved_stats = pd.read_csv(stats_file_path, index_col=0)\n",
    "#     future_table = pd.read_csv(future_table_path)\n",
    "\n",
    "    # Drop the specified columns from the future table and saved stats if they exist\n",
    "    future_table.drop(columns=excluded_columns, axis=1, errors='ignore', inplace=True)\n",
    "    saved_stats.drop(index=excluded_columns, errors='ignore', inplace=True)\n",
    "\n",
    "    # Calculate the descriptive statistics for the future table\n",
    "    future_stats = future_table.describe()\n",
    "    fout=open('qa_outputs/stats.txt',mode='w')\n",
    "    # Iterate only over the columns specified in columns_to_check\n",
    "    for column in columns_to_check:\n",
    "        if column in future_stats.columns and column in saved_stats.columns:\n",
    "            # Median comparison\n",
    "            saved_median = round(saved_stats.loc['50%', column], 2)\n",
    "            future_median = round(future_stats.loc['50%', column], 2)\n",
    "            print(f\"Column: {column}\")\n",
    "            fout.write(f\"Column: {column}\\n\")\n",
    "            print(f\"Median of {typesav}{epochsav}: {saved_median}, Median of {typen}{epoch}: {future_median}\")\n",
    "            fout.write(f\"Median of {typesav}{epochsav}: {saved_median}, Median of {typen}{epoch}: {future_median}\\n\")\n",
    "            # IQR comparison as a measure of the distribution width\n",
    "            saved_iqr = round(saved_stats.loc['75%', column] - saved_stats.loc['25%', column], 2)\n",
    "            future_iqr = round(future_stats.loc['75%', column] - future_stats.loc['25%', column], 2)\n",
    "            # print(f\"Comparing IQR for column: {column}\")\n",
    "            print(f\"IQR of {typesav}{epochsav}: {saved_iqr}, IQR of  {typen}{epoch}: {future_iqr}\")\n",
    "            fout.write(f\"IQR of {typesav}{epochsav}: {saved_iqr}, IQR of  {typen}{epoch}: {future_iqr}\\n\")\n",
    "            print(\"-\" * 50)\n",
    "            fout.write(\"-\" * 50+'\\n')\n",
    "\n",
    "\n",
    "compare_future_table_with_stats(table1data,table1_saved_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44296f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_data_1=pd.read_csv(table1_saved_stats, index_col=0)\n",
    "\n",
    "table1data=table1databackup.copy()\n",
    "stats_current_1=table1data.describe()\n",
    "\n",
    "merged_stats=pd.merge(stats_data_1, stats_current_1, left_index=True, right_index=True)\n",
    "\n",
    "merged_stats=merged_stats[['RA_x','RA_y','DEC_x','DEC_y','E_RA_x','E_RA_y','E_DEC_x','E_DEC_y','Total_flux_x','Total_flux_y','E_Total_flux_x','E_Total_flux_y','Peak_flux_x','Peak_flux_y','E_Peak_flux_x','E_Peak_flux_y','Maj_x','Maj_y','E_Maj_x','E_Maj_y','Min_x','Min_y','E_Min_x','E_Min_y','PA_x','PA_y','E_PA_x','E_PA_y','Isl_Total_flux_x','Isl_Total_flux_y','E_Isl_Total_flux_x','E_Isl_Total_flux_y','Isl_rms_x','Isl_rms_y','Isl_mean_x','Isl_mean_y','Resid_Isl_rms_x','Resid_Isl_rms_y','Resid_Isl_mean_x','Resid_Isl_mean_y','RA_max_x','RA_max_y','DEC_max_x','DEC_max_y','E_RA_max_x','E_RA_max_y','E_DEC_max_x','E_DEC_max_y','Xposn_x','Xposn_y','E_Xposn_x','E_Xposn_y','Yposn_x','Yposn_y','E_Yposn_x','E_Yposn_y','Xposn_max_x','Xposn_max_y','E_Xposn_max_x','E_Xposn_max_y','Yposn_max_x','Yposn_max_y','E_Yposn_max_x','E_Yposn_max_y','Maj_img_plane_x','Maj_img_plane_y','E_Maj_img_plane_x','E_Maj_img_plane_y','Min_img_plane_x','Min_img_plane_y','E_Min_img_plane_x','E_Min_img_plane_y','PA_img_plane_x','PA_img_plane_y','E_PA_img_plane_x','E_PA_img_plane_y','DC_Maj_x','DC_Maj_y','E_DC_Maj_x','E_DC_Maj_y','DC_Min_x','DC_Min_y','E_DC_Min_x','E_DC_Min_y','DC_PA_x','DC_PA_y','E_DC_PA_x','E_DC_PA_y','DC_Maj_img_plane_x','DC_Maj_img_plane_y','E_DC_Maj_img_plane_x','E_DC_Maj_img_plane_y','DC_Min_img_plane_x','DC_Min_img_plane_y','E_DC_Min_img_plane_x','E_DC_Min_img_plane_y','DC_PA_img_plane_x','DC_PA_img_plane_y','E_DC_PA_img_plane_x','E_DC_PA_img_plane_y','QL_image_RA_x','QL_image_RA_y','QL_image_DEC_x','QL_image_DEC_y','NVSS_distance_x','NVSS_distance_y','FIRST_distance_x','FIRST_distance_y','Peak_to_ring_x','Peak_to_ring_y','Duplicate_flag_x','Duplicate_flag_y','Quality_flag_x','Quality_flag_y','NN_dist_x','NN_dist_y','BMAJ_x','BMAJ_y','BMIN_x','BMIN_y','BPA_x','BPA_y']]\n",
    "\n",
    "# merged_stats=merged_stats[['RA_x','RA_y','DEC_x','DEC_y','E_RA_x','E_RA_y','E_DEC_x','E_DEC_y','Total_flux_x','Total_flux_y','E_Total_flux_x','E_Total_flux_y','Peak_flux_x','Peak_flux_y','E_Peak_flux_x','E_Peak_flux_y','Maj_x','Maj_y','E_Maj_x','E_Maj_y','Min_x','Min_y','E_Min_x','E_Min_y','PA_x','PA_y','E_PA_x','E_PA_y','Isl_Total_flux_x','Isl_Total_flux_y','E_Isl_Total_flux_x','E_Isl_Total_flux_y','Isl_rms_x','Isl_rms_y','Isl_mean_x','Isl_mean_y','Resid_Isl_rms_x','Resid_Isl_rms_y','Resid_Isl_mean_x','Resid_Isl_mean_y','RA_max_x','RA_max_y','DEC_max_x','DEC_max_y','E_RA_max_x','E_RA_max_y','E_DEC_max_x','E_DEC_max_y','Xposn_x','Xposn_y','E_Xposn_x','E_Xposn_y','Yposn_x','Yposn_y','E_Yposn_x','E_Yposn_y','Xposn_max_x','Xposn_max_y','E_Xposn_max_x','E_Xposn_max_y','Yposn_max_x','Yposn_max_y','E_Yposn_max_x','E_Yposn_max_y','Maj_img_plane_x','Maj_img_plane_y','E_Maj_img_plane_x','E_Maj_img_plane_y','Min_img_plane_x','Min_img_plane_y','E_Min_img_plane_x','E_Min_img_plane_y','PA_img_plane_x','PA_img_plane_y','E_PA_img_plane_x','E_PA_img_plane_y','DC_Maj_x','DC_Maj_y','E_DC_Maj_x','E_DC_Maj_y','DC_Min_x','DC_Min_y','E_DC_Min_x','E_DC_Min_y','DC_PA_x','DC_PA_y','E_DC_PA_x','E_DC_PA_y','DC_Maj_img_plane_x','DC_Maj_img_plane_y','E_DC_Maj_img_plane_x','E_DC_Maj_img_plane_y','DC_Min_img_plane_x','DC_Min_img_plane_y','E_DC_Min_img_plane_x','E_DC_Min_img_plane_y','DC_PA_img_plane_x','DC_PA_img_plane_y','E_DC_PA_img_plane_x','E_DC_PA_img_plane_y','QL_image_RA_x','QL_image_RA_y','QL_image_DEC_x','QL_image_DEC_y','NVSS_distance_x','NVSS_distance_y','FIRST_distance_x','FIRST_distance_y','Peak_to_ring_x','Peak_to_ring_y','Duplicate_flag_x','Duplicate_flag_y','Quality_flag_x','Quality_flag_y','NN_dist_x','NN_dist_y','BMAJ_x','BMAJ_y','BMIN_x','BMIN_y','BPA_x','BPA_y','Best_neuron_y_x','Best_neuron_y_y','Best_neuron_x_x','Best_neuron_x_y','Neuron_dist_x','Neuron_dist_y','P_sidelobe_x','P_sidelobe_y']]\n",
    "\n",
    "# Creating the mapping for old to new column names\n",
    "new_column_names = {col: col.replace('_x', f'_{typesav}{epochsav}').replace('_y', f'_{typen}{epoch}') for col in merged_stats.columns}\n",
    "\n",
    "# Renaming the columns\n",
    "merged_stats.rename(columns=new_column_names, inplace=True)\n",
    "merged_stats=merged_stats.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529fe928",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "# Set to display all the rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "# Optionally, set to display the full content in each cell\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563ccb9b",
   "metadata": {},
   "source": [
    "## Stats comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e25ef8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "merged_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b667961",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_stats.to_csv('qa_outputs/merged_stats.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c738156b",
   "metadata": {},
   "source": [
    "# Plotting the comparison plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2327593",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipykernel pandas matplotlib seaborn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deb5eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def plot_histograms(data, columns, output_dir, saved_counts_dir):\n",
    "    \"\"\"\n",
    "    Generate histograms for the specified columns in the dataset using the bin edges from previously saved histograms\n",
    "    and overlay the histogram from previously saved counts and bin edges.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Load the previous histogram's bin edges\n",
    "        edges_file = f\"{saved_counts_dir}/{col}_se2_bin_edges.csv\"\n",
    "        if not os.path.exists(edges_file):\n",
    "            print(f\"Bin edges file for {col} not found.\")\n",
    "            continue\n",
    "\n",
    "        bin_edges = np.loadtxt(edges_file, delimiter=\",\")\n",
    "        \n",
    "        # Filter out non-positive values if present and ensure the data fits within the bin range\n",
    "        filtered_data = data[col][(data[col] > 0) & (data[col] <= bin_edges[-1])]\n",
    "\n",
    "        # Plotting the new histogram using the loaded bin edges\n",
    "        plt.hist(filtered_data, bins=bin_edges, edgecolor='black', alpha=0.5, label=typen+epoch, density=True)\n",
    "#         print(bin_edges)\n",
    "        # Load and overlay the previous histogram\n",
    "        counts_file = f\"{saved_counts_dir}/{col}_se2_counts.csv\"\n",
    "        if os.path.exists(counts_file):\n",
    "            counts = np.loadtxt(counts_file, delimiter=\",\")\n",
    "            # Ensure that counts are normalized for the area to match\n",
    "            plt.bar(bin_edges[:-1], counts, width=np.diff(bin_edges), color='orange', edgecolor='black', align='edge', alpha=0.5, label=typesav+epochsav)\n",
    "\n",
    "        plt.title(f'Histogram of {col}')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Normalized Frequency')\n",
    "        plt.xscale('log')  # Setting the x-axis to a logarithmic scale\n",
    "        plt.yscale('log')  # Setting the y-axis to a logarithmic scale for normalized counts\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"{output_dir}/{col}_histogram.png\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Usage example - adjust the paths and directories as needed\n",
    "data = table1data\n",
    "os.system('rm -rf qa_outputs')\n",
    "os.mkdir('qa_outputs')\n",
    "output_directory = 'qa_outputs'\n",
    "\n",
    "\n",
    "saved_counts_directory = 'saved_counts'\n",
    "\n",
    "# Example columns to plot and analyze - modify as per your data\n",
    "flux_columns = ['Total_flux', 'Peak_flux']\n",
    "size_columns = ['DC_Maj']\n",
    "# spectral_index_columns = ['Spectral_index']\n",
    "quality_flag_column = 'Quality_flag'\n",
    "\n",
    "# Generating plots with overlay\n",
    "plot_histograms(data, flux_columns + size_columns, output_directory, saved_counts_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3f9633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def plot_bar_chart(data, column, output_dir, saved_counts_file):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "#     current_value_counts = data[column].value_counts().sort_index()\n",
    "    current_value_counts = data[column].value_counts(normalize=True).sort_index()\n",
    "    \n",
    "    current_value_counts.plot(kind='bar', ax=ax, color='blue', alpha=0.5, label=typen+epoch)\n",
    "\n",
    "    saved_value_counts = pd.read_csv(saved_counts_file, sep='\\t', index_col=0)#.sort_index()\n",
    "    saved_value_counts['count'] /= saved_value_counts['count'].sum()  # Normalizing\n",
    "    saved_value_counts = saved_value_counts.sort_index()\n",
    "\n",
    "    saved_value_counts.plot(kind='bar', ax=ax, color='red', alpha=0.5,label=typesav+epochsav)\n",
    "\n",
    "    plt.title(f'Bar Chart of {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Count')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{output_dir}/{column}_barchart.png\")\n",
    "    plt.show()\n",
    "# Usage example - Replace with your actual file paths\n",
    "saved_counts_file = 'saved_counts/Quality_flag_value_counts.txt'  # Path to the saved counts\n",
    "column = 'Quality_flag'  # The column to plot\n",
    "output_directory = 'qa_outputs'  # Directory to save the plot\n",
    "current_data = table1data\n",
    "\n",
    "# Generate the bar chart and overplot with saved data\n",
    "plot_bar_chart(current_data, column, output_directory, saved_counts_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d53e43",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e421309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appendix\n",
    "\n",
    "## The code which was used for saving the bin edges and counts for the histogram\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# df1=pd.read_csv('/home/biny/Dropbox/pythoncodes/Uom/uomwebsite/CIRADA_VLASS2SEv2_table1_components.csv')\n",
    "# # Assuming counts and bin_edges are obtained from the np.histogram function\n",
    "# filtered_data=df1[df1.Total_flux>0]['Total_flux']\n",
    "# # Determine the range for logspace\n",
    "# min_val = filtered_data.min()\n",
    "# max_val = filtered_data.max()\n",
    "# bins = 10**(np.linspace(np.log10(min_val), np.log10(max_val), 30))\n",
    "\n",
    "# counts, bin_edges = np.histogram(filtered_data, bins=bins, density=True)\n",
    "\n",
    "# # Save counts and bin_edges to files\n",
    "# np.savetxt(\"saved_counts/Total_flux_se2_counts.csv\", counts, delimiter=\",\", fmt='%1.15e')\n",
    "# np.savetxt(\"saved_counts/Total_flux_se2_bin_edges.csv\", bin_edges, delimiter=\",\", fmt='%1.15e')\n",
    "\n",
    "# filtered_data=df1[df1.Peak_flux>0]['Peak_flux']\n",
    "# # Determine the range for logspace\n",
    "# min_val = filtered_data.min()\n",
    "# max_val = filtered_data.max()\n",
    "# bins = 10**(np.linspace(np.log10(min_val), np.log10(max_val), 30))\n",
    "\n",
    "# counts, bin_edges = np.histogram(filtered_data, bins=bins, density=True)\n",
    "\n",
    "# # Save counts and bin_edges to files\n",
    "# np.savetxt(\"saved_counts/Peak_flux_se2_counts.csv\", counts, delimiter=\",\", fmt='%1.15e')\n",
    "# np.savetxt(\"saved_counts/Peak_flux_se2_bin_edges.csv\", bin_edges, delimiter=\",\", fmt='%1.15e')\n",
    "\n",
    "# filtered_data=df1[df1.DC_Maj>0]['DC_Maj']\n",
    "# # Determine the range for logspace\n",
    "# min_val = filtered_data.min()\n",
    "# max_val = filtered_data.max()\n",
    "# bins = 10**(np.linspace(np.log10(min_val), np.log10(max_val), 30))\n",
    "\n",
    "# counts, bin_edges = np.histogram(filtered_data, bins=bins, density=True)\n",
    "\n",
    "\n",
    "# np.savetxt(\"saved_counts/DC_Maj_se2_counts.csv\", counts, delimiter=\",\", fmt='%1.15e')\n",
    "# np.savetxt(\"saved_counts/DC_Maj_se2_bin_edges.csv\", bin_edges, delimiter=\",\", fmt='%1.15e')\n",
    "# column='Quality_flag'\n",
    "# value_counts = df1[column].value_counts()\n",
    "# value_counts.to_csv(f\"saved_counts/{column}_value_counts.txt\", sep='\\t')\n",
    "# value_counts\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
